{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention&RNN_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/hRspF8srAapiejH5I3CW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Barleysack/tensor101/blob/handsonml2_9/Attention%26RNN_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrtsJF2AUjj8"
      },
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs8NZtoPU-7c"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_qXp-jcV5Iz",
        "outputId": "3ddbb8b5-eb1c-4a2d-9573-cacb4ce102e3"
      },
      "source": [
        "print(shakespeare_text[:148])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7pJ91XjWEVS"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text) #여기서 인덱스가 1~39로 나타난다."
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtbcfV0bWUAy",
        "outputId": "314bc57b-77a7-400a-9682-492827322b3b"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TlZ3cwBWeqK",
        "outputId": "6161cdf1-74c6-4b3b-b169-bd534c3dd2b9"
      },
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
        "\n",
        "#이 토크나이저 api는 이후 프로젝트에서 출력시 사용 가능해보인다. \n",
        "#가능할까? \n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwtbpu98Wgny"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # 고유 글자 개수.\n",
        "dataset_size = tokenizer.document_count # 총 글자 개수.\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I5u_J9xW6s6"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 #여기서 0~38까지로 알아듣기 위해 1을 뺌.#encoded 배열은...이라는 뜻. 배열을 지정하는 법을 여기서 배우다니 조금 늦은 감이 없잖아 있지 않나 싶다.\n",
        "train_size = dataset_size * 90 // 100 #처음 90퍼센트를 훈련 세트로 사용한다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) #시계열 데이터는 변하지 않으리라 가정한다. 그렇지 않은 경우도 많다. encoded 배열의 원소를 글자별로 나눠 텐서로 반환하는 tf.data.Dataset 객체 생성"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0RZaeRnYfYS"
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # 타겟은 한 글자 앞의 INPUT, #window 메소드는 각각 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋을 만들어냄. 리스트의 리스트 개념이라고 보면 된다. 중첩 데이터셋의 생성.\n",
        "\"\"\"[1,2,3,4,5,6,7,8,9,10]이 있을때 윈도우 크기가 3이라면, 하기된 메서드는 [1,2,3],[2,3,4],[3,4,5].....하는 식으로 된 데이터셋을 생성하는 메서드이다.\"\"\"\n",
        "\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)#다른 데이터와 크기가 다른 윈도는 모두 쳐냄.\n",
        "#하지만 모델은 텐서를 받아야한다. 데이터셋이 아니라. 따라서 플랫-데이터셋으로 변환한다. "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkXyHEq7ePLP"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length)) #플랫 데이터셋으로 변환. 윈도마다 batch(window_length)를 호출하여 윈도 길이와 같은 텐서 하나를 다믕ㄴ 데이터셋을 얻는다. 따라서 101글자...!\n",
        "#경사하강법은 훈련세트 샘플이 동일 독립 분포일때 가장 잘 작동한다. 셔플 후 배치로 만들고 난 뒤"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwR3og99gNp-"
      },
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size) #데이터셋을 셔플한 뒤 배치로 변환한다. \n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))#입력(윈도우의 처음 100글자)와 타깃(마지막 글자)를 분리해준다. "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJZwIYQQgQDQ"
      },
      "source": [
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))#범주형 입력 특성이기에 원-핫 벡터로 인코딩한다.\n",
        "    "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYDNQZe1hywK",
        "outputId": "d9b99278-e695-4bd5-dda2-4016a8108bf3"
      },
      "source": [
        "dataset = dataset.prefetch(1) #프리페칭 추가. \n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eANjs8I0iE0f"
      },
      "source": [
        "# 이로서 데이터셋 준비 완료. 모델을 만들고 훈련한다. \n",
        "#사용 모델은 Char-RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "OeC2Rw1qiDf8",
        "outputId": "5e918575-dea4-4a7c-a6cf-190acc1059f9"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2), #recurrent_dropout=0.2), #recurrent dropout? 입력과 은닉상태 모두에게 20퍼센트 드롭아웃을 사용하겠다는 뜻. RNN의 구조에 대한 복습 필요. 128개 유닛을 가진 GRU 2개층 사용. \n",
        "                  \n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     dropout=0.2), #recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
        "                    epochs=10)\n",
        "                                 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ed78464c75ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = keras.models.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n\u001b[1;32m      3\u001b[0m                      dropout=0.2), #recurrent_dropout=0.2), #recurrent dropout? 입력과 은닉상태 모두에게 20퍼센트 드롭아웃을 사용하겠다는 뜻. RNN의 구조에 대한 복습 필요. 128개 유닛을 가진 GRU 2개층 사용. \n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     keras.layers.GRU(128, return_sequences=True,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GglfyWDF0uqK"
      },
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclpT0Y90xvB"
      },
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg50arf000Cf"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSMdq3qg01NJ"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model.predict(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D191rXt03R-"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}