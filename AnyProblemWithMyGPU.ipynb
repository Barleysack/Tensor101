{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnyProblemWithMyGPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPABb0GCRjaSbln1c3L0i4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Barleysack/tensor101/blob/handsonml2_10/AnyProblemWithMyGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BIulIPe6aM0",
        "outputId": "6ca2a51a-6c2f-4426-a230-88f039fedf21"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Imrm9627KD",
        "outputId": "9b0e82f4-e5b6-4303-d707-b51ceb570994"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "#Memory issue, again\n",
        "\"\"\"[원인] GPU Memory를 TensorFlow process가 독점하고 있는데, 다른 TensorFlow Process가 GPU Memory에 접근하려고 할때 발생하는 에러.\n",
        "\n",
        "\n",
        "\n",
        "[해결책] TensorFlow Session 설정에서 GPU 메모리를 공유하도록 설정\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "path_to_zip = '/gdrive/My Drive/'\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/kor.txt\"\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성합니다.\n",
        "  # 예시: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n",
        "  w = re.sub(r\"[^a-zA-Z가-힣?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n",
        "  # 문장에 start와 end 토큰을 추가합니다.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1.특정 문자를 제거함으로써 문장을 정리합니다.\n",
        "2.각 문장에 start와 end 토큰을 추가합니다.\"\"\"   \n",
        "def preprocess_sentence_kr(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        " \n",
        "  w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', \" \", w)\n",
        "#한글은 알파벳이 아니지. 그래서 전처리가 불가능했다. \n",
        "  w = w.strip()\n",
        "\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "\"\"\"\n",
        "3.단어 인덱스와 아이디(ID) 인덱스를 생성합니다. (단어 → 아이디(ID), 아이디(ID) → 단어로 매핑된 딕셔너리).\n",
        "4.각 문장을 입력층의 최대 길이만큼 패딩(padding)을 수행합니다.\"\"\"\n",
        "\n",
        "\n",
        "#데이터셋 토큰화.\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')#따옴표를 기준으로 토큰화\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "#\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "#최대길이만큼 패딩 생성\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "  data = pd.read_csv(path_to_file, delimiter = \"\\t\")\n",
        "  data.columns = [\"en\", \"kor\", \"cc\"]\n",
        "  en = [preprocess_sentence(l) for l in data['en']]\n",
        "  kr = [preprocess_sentence_kr(l) for l in data['kor']]\n",
        "  return en, kr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 유니코드 파일을 아스키 코드 파일로 변환합니다.\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "\n",
        "num_examples = 3000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "en, kr = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(kr[-1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])\n",
        "\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "   \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "   \n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        " \n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "  \n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
        "    \n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    \n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "   \n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "   \n",
        "    x = self.embedding(x)\n",
        "\n",
        "   \n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "  \n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    \n",
        "    for t in range(1, targ.shape[1]):\n",
        "     \n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss\n",
        "\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence_kr(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        " \n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot\n",
        "\n",
        "\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
            "<start> 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지 . 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면 , 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고 , 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면 , 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면 , 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야 . <end>\n",
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> 톰이\n",
            "963 ----> 거짓말했어\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5 ----> tom\n",
            "355 ----> lied\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 97, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 97, 1)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 2484)\n",
            "Epoch 1 Batch 0 Loss 0.5769\n",
            "Epoch 1 Loss 0.3975\n",
            "Time taken for 1 epoch 188.9957549571991 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3470\n",
            "Epoch 2 Loss 0.3288\n",
            "Time taken for 1 epoch 75.64460492134094 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.3005\n",
            "Epoch 3 Loss 0.2988\n",
            "Time taken for 1 epoch 76.97641491889954 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2598\n",
            "Epoch 4 Loss 0.2687\n",
            "Time taken for 1 epoch 78.60604047775269 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2672\n",
            "Epoch 5 Loss 0.2442\n",
            "Time taken for 1 epoch 78.08540439605713 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2147\n",
            "Epoch 6 Loss 0.2241\n",
            "Time taken for 1 epoch 78.50659322738647 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2159\n",
            "Epoch 7 Loss 0.2067\n",
            "Time taken for 1 epoch 78.19683027267456 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1796\n",
            "Epoch 8 Loss 0.1901\n",
            "Time taken for 1 epoch 78.40161490440369 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1645\n",
            "Epoch 9 Loss 0.1723\n",
            "Time taken for 1 epoch 77.90601229667664 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1542\n",
            "Epoch 10 Loss 0.1547\n",
            "Time taken for 1 epoch 78.52766013145447 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb5f896d490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md0Jy_rd5M9f"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QVgPcDP1_iMz",
        "outputId": "c2caa4bb-c025-4782-c8d5-8761a73090c4"
      },
      "source": [
        "translate(u'오늘 날씨가 좋다')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> 오늘 날씨가 좋다 <end>\n",
            "Predicted translation: the price are meowing . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4363 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4457 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4354 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4467 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4527 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4449 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4362 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4469 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4352 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4364 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4546 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 4355 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4363 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4457 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4354 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4467 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4527 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4449 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4362 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4469 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4352 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4364 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4546 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 4355 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg3kdyUS_5Jq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}